{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9896,"sourceType":"datasetVersion","datasetId":6302},{"sourceId":47853,"sourceType":"datasetVersion","datasetId":35388}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#%pip install lxml\nimport lxml\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport xml.etree.ElementTree as et\nimport os\nimport matplotlib.pyplot as pt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T10:11:43.701072Z","iopub.execute_input":"2024-08-29T10:11:43.701671Z","iopub.status.idle":"2024-08-29T10:11:43.708797Z","shell.execute_reply.started":"2024-08-29T10:11:43.701621Z","shell.execute_reply":"2024-08-29T10:11:43.707112Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#read xml file\n\nanno=\"/kaggle/input/pascal-voc-2012/VOC2012/Annotations/\"\ndef read_xml_file(data):\n    tree=et.parse(data)\n    root=tree.getroot()\n    image_name=root.find('filename').text\n    \n    image_width=int(root.find(\"size\").find(\"width\").text)\n    image_height=int(root.find(\"size\").find(\"height\").text)\n  #  image_depth=root.find(\"size\").find(\"depth\").text\n    new_width=224/image_width\n    new_heigh=224/image_height\n  #  image_size=(int(image_width),int(image_height))\n    \n    b_box=[]\n    names=[]\n    for i in root.findall(\"object\"):\n        name=i.find(\"name\").text\n        \n        names.append(name)\n        x_min=float(i.find(\"bndbox\").find(\"xmin\").text)*new_width\n        y_min=float(i.find(\"bndbox\").find(\"ymin\").text)*new_heigh\n        x_max=float(i.find(\"bndbox\").find(\"xmax\").text)*new_width\n        y_max=float(i.find(\"bndbox\").find(\"ymax\").text)*new_heigh\n        b_box.append((x_min,y_min,x_max,y_max))\n    \n    return(image_name,names,b_box)\n    \ndict_bbox={}\n\nfor i in os.listdir(anno):\n    image_name,name,b_box=read_xml_file(anno+i)\n    dict_bbox[image_name]=(name,b_box)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T10:11:43.732383Z","iopub.execute_input":"2024-08-29T10:11:43.732956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read all images and label bounding box \ndef read_image(image):\n    im=cv2.imread(image)\n    im=cv2.resize(im,(224,224))\n    return im\n\ntrain=[]\nlabel=[]\nimage=\"/kaggle/input/pascal-voc-2012/VOC2012/JPEGImages/\"\nimages=[]\nfor i in os.listdir(image):\n    train.append(read_image(image+i))\n    label.append(dict_bbox[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build vgg16 as backbone for faster rcnn\nimport tensorflow as tf\nfrom keras.layers import Conv2D,MaxPool2D,Dense,Flatten\nfrom keras.models import Sequential\n\nmodel=Sequential()\n\nmodel.add(Conv2D(kernel_size=(3,3),filters=64,input_shape=(224,224,3),activation='relu',padding='same',name='block1_conv1'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=64,activation='relu',padding='same',name='block1_conv2'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name= 'block1_pool'))\n\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu',padding='same',name= 'block2_conv1'))\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu',padding='same',name= 'block2_conv2',))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='block2_pool'))\n\nmodel.add(Conv2D(kernel_size=(3,3),filters=256,activation='relu',padding='same',name='block3_conv1'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=256,activation='relu',padding='same',name='block3_conv2'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=256,activation='relu',padding='same',name='block3_conv3'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='block3_pool'))\n\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block4_conv1'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block4_conv2'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block4_conv3'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='block4_pool'))\n\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block5_conv1'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block5_conv2'))\nmodel.add(Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',name='block5_conv3'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2),name='block5_pool'))\n\npath=\"/kaggle/input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n\nmodel.load_weights(path,by_name=True)\nmodel.compile(loss='cross_entropy',optimizer=\"adam\")\nmodel.summary()\n\nOUT_VGG16=model.outputs\n\nprint(OUT_VGG16)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build RPN to faster rcnn\nfrom tensorflow.keras.models import Model\n\ndef rpn_model(num_anchor,model_use):\n    \n    conv=Conv2D(kernel_size=(3,3),filters=512,activation='relu',padding='same',strides=(1,1))(model_use)\n    \n    rpn_cls=Conv2D(filters=num_anchor*2,kernel_size=(1,1),activation='softmax')(conv)\n    \n    rpn_reg=Conv2D(filters=num_anchor*4,kernel_size=(1,1))(conv)\n    \n    #return Model(inputs=model_use,outputs=[anch,anc_reg])\n    return rpn_cls,rpn_reg\n\n\nrpn_cls, rpn_reg=rpn_model(9,OUT_VGG16[0])\n#print(rbn.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create ROIS File for Faster RCNN\n\ndef create_roi(bound_box,ima_num,batch):\n    #batch_index=0\n    batch_index=ima_num//batch\n    roi=[]\n    for i in bound_box:\n        x1=i[0]/224\n        y1=i[1]/224\n        x2=i[2]/224\n        y2=i[3]/224\n        roi.append(np.array([batch_index,y1,x1,y2,x2]))\n        \n    return roi\n\n#print(\"=============================================================\")\nroi_arr=[]\ncount=0\nfor i in(label):\n    \n    roi_arr.append(create_roi(i[1],count,50))\n\n    count+=1\n#print(label[0][1])\nprint(roi_arr[0])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create roi align layer \n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Lambda\n\ndef roi_align(feature_map,rois,out_size=(7,7)):\n    \n    boxs=tf.convert_to_tensor(rois,dtype=tf.float32)\n    \n    crob=tf.image.crob_and_resize(feature_map,boxs,box_indices=boxs[:,0],crop_size=out_size)\n    \n    return crob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we concatenate every thing \nfrom tensorflow.keras.layers import Input,Lambda\n\n#input_image=Input(shape=(None,None,3))\n\nfeture=OUT_VGG16[0]\n\nrois=roi_arr\n\nrpn_cls, rpn_reg=rpn_cls, rpn_reg\n\nalign_rois=Lambda(lambda x:roi_align(x[0],x[1],x[2]),output_shape=(None,7,7,512))([feture,rois])\n\nmodel=Model(inputs=input_image,outputs=[rpn_cls, rpn_reg,align_rois])\nmodel.compile(optimizer='adam', loss={'classification': 'categorical_crossentropy', 'regression': 'mean_squared_error', 'output': 'categorical_crossentropy'})\n\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this to edit h5 file we donit use it in the code we just trye\nimport h5py\nimport shutil\n\ndata_h5=\"/kaggle/input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n#data_h5_work=\"/kaggle/working/vgg15.h5\"\n\n#shutil.copy(data_h5, data_h5_work)\n\nfile=h5py.File(data_h5)\nprint(list(file.keys()))\n#with h5py.File(data_h5,'r+') as f:\n    \"\"\"if 'fc1' in f:\n        del(f['fc1'])\n    if 'fc2' in f:\n        del(f['fc2'])\n    if 'flatten' in f:\n        del(f['flatten'])\n    if 'predictions' in f:\n        del(f['predictions'])\"\"\"\n    \n    #print(list(f.keys()))\n    \n    \n#file=h5py.File(data_h5_work)\n#print(list(file.keys()))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda, Reshape\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\n# Define VGG16 as feature extractor\n#def build_feature_extractor(input_shape):\nbase_model = VGG16(include_top=False, input_shape=input_shape)\nprint(base_model.outputs)\nbase_model.summary()\n   # x = base_model.output\n#    return Model(inputs=base_model.input, outputs=x)\n\n#input_shape = (None, None, 3)  # Image shape\n#feature_extractor = build_feature_extractor(input_shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}